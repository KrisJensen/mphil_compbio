\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Unsupervised learning}{1}}
\newlabel{eq:batchlearn}{{2}{1}}
\newlabel{eq:cov}{{3}{1}}
\newlabel{eq:anal}{{5}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Multiplicative normalization}{1}}
\newlabel{eq:ojacont}{{6}{1}}
\newlabel{eq:oja}{{8}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sim1mul}{{1a}{2}}
\newlabel{sub@fig:sim1mul}{{a}{2}}
\newlabel{fig:sim2mul}{{1b}{2}}
\newlabel{sub@fig:sim2mul}{{b}{2}}
\newlabel{fig:sim3mul}{{1c}{2}}
\newlabel{sub@fig:sim3mul}{{c}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learning weight vectors from 500 input data points with Oja's rule. In each case, we start from [w1, w2] = [0.001, 0.001] and iterate through equation 8\hbox {} until convergence using either a correlation or covariance-based learning rule. Red dotted lines indicate the direction of the final weight vector. All datasets were generated with a slope of 1, a correlation of $\rho = -0.7$, and standard deviations of 1 and $1-\rho ^2$ in the $u_1$ and $u_2$ directions. \relax }}{2}}
\newlabel{fig:multiplicative}{{1}{2}}
\newlabel{fig:sim1vec}{{2a}{3}}
\newlabel{sub@fig:sim1vec}{{a}{3}}
\newlabel{fig:sim2vec}{{2b}{3}}
\newlabel{sub@fig:sim2vec}{{b}{3}}
\newlabel{fig:sim3vec}{{2c}{3}}
\newlabel{sub@fig:sim3vec}{{c}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Weight vector magnitude as a function of iteration number for the three simulations in figure 1\hbox {}.\relax }}{3}}
\newlabel{fig:multiplicativevec}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Subtractive normalization}{3}}
\newlabel{eq:subcont}{{9}{3}}
\newlabel{eq:subavg}{{11}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Histograms of angles between the first PC vector and the weight vector converged to from a random initial vector when the data has a slope of $1$ (a) or $0.2$ (b). We do not recover the first principal component since $\theta \not =0$.\relax }}{4}}
\newlabel{fig:thetas}{{3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Ocular dominance columns}{4}}
\newlabel{eq:occont}{{13}{4}}
\newlabel{eq:ocheb}{{15}{4}}
\newlabel{eq:ocdic}{{16}{4}}
\newlabel{eq:ocdic2}{{18}{5}}
\newlabel{eq:subnorm}{{20}{5}}
\newlabel{eq:cortint}{{21}{5}}
\newlabel{fig:plotK}{{4a}{5}}
\newlabel{sub@fig:plotK}{{a}{5}}
\newlabel{fig:stds}{{4b}{5}}
\newlabel{sub@fig:stds}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax }}{5}}
\newlabel{}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces ${\bf  w}_-$ as a heatmap after 50, 200, 300 and 1000 iterations. We see that ocular dominance becomes increasingly strong over time and that the short-range excitation with long-range inhibition drives the formation of an oscillatory pattern of dominance.\relax }}{6}}
\newlabel{fig:occsim_int}{{5}{6}}
\newlabel{fig:Ktilde}{{6a}{6}}
\newlabel{sub@fig:Ktilde}{{a}{6}}
\newlabel{fig:eigvec}{{6b}{6}}
\newlabel{sub@fig:eigvec}{{b}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax }}{6}}
\newlabel{}{{6}{6}}
\newlabel{fig:DFT}{{7a}{7}}
\newlabel{sub@fig:DFT}{{a}{7}}
\newlabel{fig:maxk}{{7b}{7}}
\newlabel{sub@fig:maxk}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax }}{7}}
\newlabel{}{{7}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Simulations of the ocular dominance system for different values of $\sigma $ corresponding to different interaction ranges. Longer-range interactions lead to wider stripes.\relax }}{7}}
\newlabel{fig:sigs}{{8}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The elastic net}{8}}
\newlabel{eq:update}{{25}{8}}
\newlabel{eq:energy}{{27}{8}}
\newlabel{fig:Ls}{{9a}{9}}
\newlabel{sub@fig:Ls}{{a}{9}}
\newlabel{fig:Ts}{{9b}{9}}
\newlabel{sub@fig:Ts}{{b}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Performance (left) and computational time (right) for the elastic net approach to the travelling salesman problem as a function of decay rate $\lambda $ and number of points on the path M. All simulations were run on the same set of 100 cities with remaining parameters as specified in the main text.\relax }}{9}}
\newlabel{fig:tspopt}{{9}{9}}
\newlabel{fig:sim1}{{10a}{9}}
\newlabel{sub@fig:sim1}{{a}{9}}
\newlabel{fig:sim2}{{10b}{9}}
\newlabel{sub@fig:sim2}{{b}{9}}
\newlabel{fig:sim3}{{10c}{9}}
\newlabel{sub@fig:sim3}{{c}{9}}
\newlabel{fig:sim4}{{10d}{9}}
\newlabel{sub@fig:sim4}{{d}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Timecourse of the travelling salesman simulation with M=1.5 and $\lambda = 0.0005$\relax }}{9}}
\newlabel{fig:tspsim}{{10}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance and computational time for the elastic net and simulated annealing across three different maps of 100 randomly generated cities. For simulated annealing, results are reported as mean (std).\relax }}{10}}
\newlabel{tab:tsp}{{1}{10}}
\newlabel{fig:comp1el}{{11a}{10}}
\newlabel{sub@fig:comp1el}{{a}{10}}
\newlabel{fig:comp1an}{{11b}{10}}
\newlabel{sub@fig:comp1an}{{b}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{10}}
\newlabel{fig:comp1}{{11}{10}}
\newlabel{fig:comp2el}{{12a}{10}}
\newlabel{sub@fig:comp2el}{{a}{10}}
\newlabel{fig:comp2an}{{12b}{10}}
\newlabel{sub@fig:comp2an}{{b}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces For map 2, the optimum solution found by simulated annealing is more symmetrical, and the elastic net comes close to the performance of simulated annealing.\relax }}{10}}
\newlabel{fig:comp2}{{12}{10}}
\newlabel{eq:update2}{{28}{11}}
\newlabel{fig:dom05}{{13a}{11}}
\newlabel{sub@fig:dom05}{{a}{11}}
\newlabel{fig:dom075}{{13b}{11}}
\newlabel{sub@fig:dom075}{{b}{11}}
\newlabel{fig:dom100}{{13c}{11}}
\newlabel{sub@fig:dom100}{{c}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Ocular dominance maps resulting from a two-dimensional elastic net simulation. Individual squares (cortical neurons) are coloured according to whether they are most closely associated with the right (white) or left (black) retina. Stripe width increases with $l/d$.\relax }}{11}}
\newlabel{fig:widths}{{13}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Retinal-to-cortical mapping in the retinal x direction at the beginning (left) and end (right) of a simulation with $2l = 0.15$ and $2d = 0.05$. x and y axes correspond to locations in the cortex. Colours indicate retinal connectivity (i.e. location in retinal space) with white and black regions corresponding to cortical neurons with input from retinal neurons with low and high x-values respectively. We see that neurons self-organize from the random initial connectivity to generate a cortical map.\relax }}{12}}
\newlabel{fig:xs_075}{{14}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Cart pole balancing problem}{12}}
\newlabel{eq:impulse}{{29}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Cart pole balancing simulation showing x (left) and theta (right) over 1000 seconds of simulated time.\relax }}{13}}
\newlabel{fig:balance_example}{{15}{13}}
\newlabel{fig:earlycart}{{16a}{13}}
\newlabel{sub@fig:earlycart}{{a}{13}}
\newlabel{fig:learned}{{16b}{13}}
\newlabel{sub@fig:learned}{{b}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces \relax }}{13}}
\newlabel{fig:balancing}{{16}{13}}
\newlabel{fig:singletrial}{{17a}{14}}
\newlabel{sub@fig:singletrial}{{a}{14}}
\newlabel{fig:500trials}{{17b}{14}}
\newlabel{sub@fig:500trials}{{b}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Plots of trial duration (simulated time) against trial number. We see that for a single trial, there is a very steep increase in trial duration after the final failure, and over 500 trials this averages out to approximately linear learning between trial 30 and 70. By 100 trials, there are generally no more failures.\relax }}{14}}
\newlabel{fig:cartpole_learning}{{17}{14}}
\newlabel{fig:delta_lambda}{{18a}{14}}
\newlabel{sub@fig:delta_lambda}{{a}{14}}
\newlabel{fig:alpha_beta}{{18b}{14}}
\newlabel{sub@fig:alpha_beta}{{b}{14}}
\newlabel{fig:mp_l}{{18c}{14}}
\newlabel{sub@fig:mp_l}{{c}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Learning performance as a function of different sets of parameters. The value given is the number of trials needed for the mean trial length to exceed 1000 seconds of simulated time. Lower values indicate better performance. The simulations were capped at an upper limit of 200 trials and all other parameters were fixed at their default values.\relax }}{14}}
\newlabel{fig:params}{{18}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Performance of the Anderson network using a network with one or two layers. x-axis indicates trial number and y-axis indicates trial length in simulated time.\relax }}{15}}
\newlabel{fig:cart_layers}{{19}{15}}
\@writefile{lol}{\contentsline {lstlisting}{unsupervised\textunderscore learning.jl}{16}}
\@writefile{lol}{\contentsline {lstlisting}{ocular\textunderscore dominance.jl}{19}}
\@writefile{lol}{\contentsline {lstlisting}{TSP.jl}{22}}
\@writefile{lol}{\contentsline {lstlisting}{elastic\textunderscore net\textunderscore OD.jl}{26}}
\@writefile{lol}{\contentsline {lstlisting}{cart\textunderscore pole.jl}{30}}
