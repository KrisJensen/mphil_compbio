\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Unsupervised learning}{1}}
\newlabel{eq:batchlearn}{{4}{1}}
\newlabel{eq:cov}{{5}{1}}
\newlabel{eq:anal}{{8}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Multiplicative normalization}{2}}
\newlabel{eq:ojacont}{{9}{2}}
\newlabel{eq:oja}{{11}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Subtractive normalization}{2}}
\newlabel{eq:subcont}{{12}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sim1mul}{{1a}{3}}
\newlabel{sub@fig:sim1mul}{{a}{3}}
\newlabel{fig:sim2mul}{{1b}{3}}
\newlabel{sub@fig:sim2mul}{{b}{3}}
\newlabel{fig:sim3mul}{{1c}{3}}
\newlabel{sub@fig:sim3mul}{{c}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learning weight vectors from 500 input data points with Oja's rule. In each case, we start from [w1, w2] = [0.001, 0.001] and iterate through equation 11\hbox {} until convergence using either a correlation or covariance-based learning rule. Red dotted lines indicate the direction of the final weight vector. All datasets were generated with a standard deviation of 1 and $1-\rho ^2$ in the $u1$ and $u2$ directions and a correlation of $\rho = -0.7$.\relax }}{3}}
\newlabel{fig:multiplicative}{{1}{3}}
\newlabel{fig:sim1vec}{{2a}{3}}
\newlabel{sub@fig:sim1vec}{{a}{3}}
\newlabel{fig:sim2vec}{{2b}{3}}
\newlabel{sub@fig:sim2vec}{{b}{3}}
\newlabel{fig:sim3vec}{{2c}{3}}
\newlabel{sub@fig:sim3vec}{{c}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Weight vector magnitue as a function of iteration number for the three simulations in figure \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:multiplictive' on page 3 undefined}\relax }}{3}}
\newlabel{fig:multiplicativevec}{{2}{3}}
\newlabel{eq:subavg}{{14}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Ocular dominance columns}{4}}
\newlabel{eq:occont}{{15}{4}}
\newlabel{eq:ocheb}{{18}{4}}
\newlabel{eq:ocdic}{{19}{4}}
\newlabel{eq:ocdic2}{{22}{5}}
\newlabel{eq:subnorm}{{25}{5}}
\newlabel{eq:cortint}{{26}{5}}
\newlabel{fig:plotK}{{3a}{5}}
\newlabel{sub@fig:plotK}{{a}{5}}
\newlabel{fig:stds}{{3b}{5}}
\newlabel{sub@fig:stds}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax }}{5}}
\newlabel{}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ${\bf  w}_-$ as a heatmap after 50, 200, 300 and 1000 iterations. We see that ocular dominance becomes increasingly strong over time and that the short-range excitation with long-range inhibition drives the formation of an oscillatory pattern of dominance.\relax }}{6}}
\newlabel{fig:occsim_int}{{4}{6}}
\newlabel{fig:Ktilde}{{5a}{7}}
\newlabel{sub@fig:Ktilde}{{a}{7}}
\newlabel{fig:eigvec}{{5b}{7}}
\newlabel{sub@fig:eigvec}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax }}{7}}
\newlabel{}{{5}{7}}
\newlabel{fig:DFT}{{6a}{7}}
\newlabel{sub@fig:DFT}{{a}{7}}
\newlabel{fig:maxk}{{6b}{7}}
\newlabel{sub@fig:maxk}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax }}{7}}
\newlabel{}{{6}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Simulations of the ocular dominance system for different values of $\sigma $ corresponding to different interaction ranges. Longer-range interactions lead to wider stripes.\relax }}{8}}
\newlabel{fig:sigs}{{7}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The elastic net}{8}}
\newlabel{eq:update}{{32}{8}}
\newlabel{eq:energy}{{35}{8}}
\newlabel{fig:Ls}{{8a}{9}}
\newlabel{sub@fig:Ls}{{a}{9}}
\newlabel{fig:Ts}{{8b}{9}}
\newlabel{sub@fig:Ts}{{b}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Performance and computational time for the elastic net approach to the travelling salesman problem as a function of decay rate $\lambda $ and number of points on the path M. All simulations were run on the same set of 100 cities with remaining parameters as specified in the main text.\relax }}{9}}
\newlabel{fig:tspopt}{{8}{9}}
\newlabel{fig:sim1}{{9a}{10}}
\newlabel{sub@fig:sim1}{{a}{10}}
\newlabel{fig:sim2}{{9b}{10}}
\newlabel{sub@fig:sim2}{{b}{10}}
\newlabel{fig:sim3}{{9c}{10}}
\newlabel{sub@fig:sim3}{{c}{10}}
\newlabel{fig:sim4}{{9d}{10}}
\newlabel{sub@fig:sim4}{{d}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Timecourse of the travelling salesman simulation with M=1.5 and $\lambda = 0.0005$\relax }}{10}}
\newlabel{fig:tspsim}{{9}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance and computational time for the elastic net and simulated annealing across three different maps of 100 randomly generated cities. For simulated annealing, the numbers in brackets represent standard deviations from 30 trials.\relax }}{10}}
\newlabel{tab:tsp}{{1}{10}}
\newlabel{fig:comp1el}{{10a}{11}}
\newlabel{sub@fig:comp1el}{{a}{11}}
\newlabel{fig:comp1an}{{10b}{11}}
\newlabel{sub@fig:comp1an}{{b}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{11}}
\newlabel{fig:comp1}{{10}{11}}
\newlabel{fig:comp2el}{{11a}{11}}
\newlabel{sub@fig:comp2el}{{a}{11}}
\newlabel{fig:comp2an}{{11b}{11}}
\newlabel{sub@fig:comp2an}{{b}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces For map 2, the solutions found by simulated annealing are more symmetrical, and the elastic net comes close to the performance of simulated annealing.\relax }}{11}}
\newlabel{fig:comp2}{{11}{11}}
\newlabel{eq:update}{{38}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Cart pole balancing problem}{11}}
\newlabel{fig:comp1el}{{12a}{13}}
\newlabel{sub@fig:comp1el}{{a}{13}}
\newlabel{fig:comp1an}{{12b}{13}}
\newlabel{sub@fig:comp1an}{{b}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{13}}
\newlabel{fig:comp1}{{12}{13}}
\newlabel{fig:comp1el}{{13a}{13}}
\newlabel{sub@fig:comp1el}{{a}{13}}
\newlabel{fig:comp1an}{{13b}{13}}
\newlabel{sub@fig:comp1an}{{b}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{13}}
\newlabel{fig:comp1}{{13}{13}}
\newlabel{fig:comp1el}{{14a}{13}}
\newlabel{sub@fig:comp1el}{{a}{13}}
\newlabel{fig:comp1an}{{14b}{13}}
\newlabel{sub@fig:comp1an}{{b}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{13}}
\newlabel{fig:comp1}{{14}{13}}
\newlabel{fig:comp1el}{{15a}{14}}
\newlabel{sub@fig:comp1el}{{a}{14}}
\newlabel{fig:comp1an}{{15b}{14}}
\newlabel{sub@fig:comp1an}{{b}{14}}
\newlabel{fig:comp1an}{{15c}{14}}
\newlabel{sub@fig:comp1an}{{c}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Optimum routes for map 1 found by the elastic net method and simulated annealing. Simulated annealing finds a shorter route than the elastic net, and this route involves only visiting the inner region of the map once. \relax }}{14}}
\newlabel{fig:comp1}{{15}{14}}
